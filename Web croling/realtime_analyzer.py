import pandas as pd
import numpy as np
import joblib
import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime
import yfinance as yf
import re
import os

# =========================================================
# ê²½ë¡œ ì„¤ì • (ëª¨ë¸ íŒŒì¼ ì•ˆì •ì  ë¡œë“œìš©)
# =========================================================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = BASE_DIR   # ëª¨ë¸ íŒŒì¼ë“¤ì´ í˜„ì¬ í´ë”(Web croling)ì— ìˆìŒ


# =========================================================
# 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ (train_model.pyì™€ ë™ì¼)
# =========================================================
def simple_preprocess(text):
    """í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€"""
    if pd.isna(text):
        return ""
    
    text = re.sub(r'<[^>]+>', '', str(text))
    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def count_sentiment_keywords(text):
    """ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ê°œìˆ˜"""
    text_lower = str(text).lower()
    
    positive_keywords = [
        'ìƒìŠ¹', 'ì¦ê°€', 'ì„±ì¥', 'í˜¸ì¡°', 'ê°œì„ ', 'í™•ëŒ€', 'ì‹ ê¸°ë¡', 'ìµœê³ ',
        'ë§¤ì¶œì¦ê°€', 'ì´ìµì¦ê°€', 'ì‹¤ì ê°œì„ ', 'ìˆ˜ì£¼', 'ê³„ì•½', 'í˜‘ë ¥',
        'íˆ¬ì', 'ê°œë°œ', 'ì¶œì‹œ', 'ì„±ê³µ', 'ë‹¬ì„±', 'ëŒíŒŒ', 'í˜¸ì‹¤ì ', 'ê¸‰ë“±'
    ]
    
    negative_keywords = [
        'í•˜ë½', 'ê°ì†Œ', 'ì•…í™”', 'ë¶€ì§„', 'ì ì', 'ì†ì‹¤', 'ìœ„ê¸°', 'ë¦¬ìŠ¤í¬',
        'ì§€ì—°', 'ì² ìˆ˜', 'ì¤‘ë‹¨', 'ì‹¤íŒ¨', 'ë¶€ì¡±', 'ìš°ë ¤', 'í•˜í–¥', 'ê°ì›',
        'ì†Œì†¡', 'ì œì¬', 'ê·œì œ', 'ì¡°ì‚¬', 'ì ë°œ', 'ê¸‰ë½', 'í­ë½'
    ]
    
    pos_count = sum(1 for kw in positive_keywords if kw in text_lower)
    neg_count = sum(1 for kw in negative_keywords if kw in text_lower)
    
    return pos_count, neg_count

# =========================================================
# 2. ëª¨ë¸ ë¡œë“œ ë° ë¶„ì„ í´ë˜ìŠ¤
# =========================================================
class NewsAnalyzer:
    def __init__(self):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            self.sentiment_model = joblib.load(os.path.join(MODEL_DIR, 'sentiment_model.pkl'))
            self.trading_model = joblib.load(os.path.join(MODEL_DIR, 'trading_model.pkl'))
            self.tfidf_vectorizer = joblib.load(os.path.join(MODEL_DIR, 'tfidf_vectorizer.pkl'))
            self.trading_vectorizer = joblib.load(os.path.join(MODEL_DIR, 'trading_vectorizer.pkl'))
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except FileNotFoundError as e:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("   ë¨¼ì € train_model.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”.")
            raise
        
        # í‹°ì»¤ ë§¤í•‘
        self.ticker_map = {
            "ì‚¼ì„±ì „ì": "005930.KS",
            "SKí•˜ì´ë‹‰ìŠ¤": "000660.KS",
            "LGì—ë„ˆì§€ì†”ë£¨ì…˜": "373220.KS",
            "í˜„ëŒ€ì°¨": "005380.KS",
            "ê¸°ì•„": "000270.KS",
            "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤": "207940.KS",
            "ì…€íŠ¸ë¦¬ì˜¨": "068270.KS",
            "ì¹´ì¹´ì˜¤": "035720.KS",
            "ë„¤ì´ë²„": "035420.KS",
            "POSCOí™€ë”©ìŠ¤": "005490.KS"
        }
    
    # -----------------------------------------------------
    # ë‰´ìŠ¤ ë‹¨ê±´ ë¶„ì„
    # -----------------------------------------------------
    def analyze_news(self, title, content):
        """ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„"""
        combined_text = f"{title} {content}"
        cleaned = simple_preprocess(combined_text)
        pos_count, neg_count = count_sentiment_keywords(cleaned)
        
        # ================= ê°ì„± ë¶„ì„ =================
        X_tfidf = self.tfidf_vectorizer.transform([cleaned])
        X_extra = np.array([[pos_count, neg_count, 0]])  # ìˆ˜ìµë¥ ì€ 0
        X_sentiment = np.hstack([X_tfidf.toarray(), X_extra])
        
        sentiment_raw = self.sentiment_model.predict(X_sentiment)[0]
        sentiment_proba = self.sentiment_model.predict_proba(X_sentiment)[0]
        sentiment_confidence = max(sentiment_proba)
        
        # ë¼ë²¨ ë§¤í•‘ (ëª¨ë¸ í•™ìŠµ ê¸°ì¤€ì— ë§ê²Œ ì¡°ì • ê°€ëŠ¥)
        sentiment_map = {0: "ë¶€ì •", 1: "ì¤‘ë¦½", 2: "ê¸ì •"}
        sentiment = sentiment_map.get(sentiment_raw, sentiment_raw)
        
        # ================= ê±°ë˜ ì‹ í˜¸ =================
        X_trading_tfidf = self.trading_vectorizer.transform([cleaned])
        X_trading_extra = np.array([[pos_count, neg_count, 0]])
        X_trading = np.hstack([X_trading_tfidf.toarray(), X_trading_extra])
        
        trade_raw = self.trading_model.predict(X_trading)[0]
        trade_proba = self.trading_model.predict_proba(X_trading)[0]
        trade_confidence = max(trade_proba)
        
        signal_map = {0: "ë§¤ë„", 1: "ê´€ë§", 2: "ë§¤ìˆ˜"}
        trade_signal = signal_map.get(trade_raw, trade_raw)
        
        return {
            'ê°ì„±': sentiment,
            'ê°ì„±_ì‹ ë¢°ë„': f"{sentiment_confidence:.1%}",
            'ê¸ì •í‚¤ì›Œë“œ': pos_count,
            'ë¶€ì •í‚¤ì›Œë“œ': neg_count,
            'ê±°ë˜ì‹ í˜¸': trade_signal,
            'ì‹ í˜¸_ì‹ ë¢°ë„': f"{trade_confidence:.1%}",
            'ë¶„ì„ì‹œê°': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    # -----------------------------------------------------
    # í˜„ì¬ ì£¼ê°€ ì¡°íšŒ
    # -----------------------------------------------------
    def get_current_price(self, company):
        ticker = self.ticker_map.get(company)
        if not ticker:
            return None
        
        try:
            stock = yf.Ticker(ticker)
            info = stock.info
            
            current_price = info.get('currentPrice', info.get('regularMarketPrice', 0))
            prev_close = info.get('previousClose', 0)
            
            if prev_close:
                change_pct = ((current_price - prev_close) / prev_close) * 100
            else:
                change_pct = 0
            
            return {
                'í˜„ì¬ê°€': f"{current_price:,.0f}ì›",
                'ì „ì¼ëŒ€ë¹„': f"{change_pct:+.2f}%"
            }
        except:
            return None

# =========================================================
# 3. ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ & ë¶„ì„
# =========================================================
def collect_and_analyze_latest_news(company, max_news=5):
    """íŠ¹ì • ê¸°ì—…ì˜ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ” [{company}] ìµœì‹  ë‰´ìŠ¤ ë¶„ì„")
    print(f"{'='*70}")
    
    analyzer = NewsAnalyzer()
    
    # í˜„ì¬ ì£¼ê°€
    price_info = analyzer.get_current_price(company)
    if price_info:
        print(f"\nğŸ“Š í˜„ì¬ ì£¼ê°€: {price_info['í˜„ì¬ê°€']} ({price_info['ì „ì¼ëŒ€ë¹„']})")
    
    # Google News RSS
    encoded = urllib.parse.quote(company)
    rss_url = f"https://news.google.com/rss/search?q={encoded}&hl=ko&gl=KR&ceid=KR:ko"
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        res = requests.get(rss_url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.content, "xml")
        items = soup.find_all("item")[:max_news]
        
        if not items:
            print("âš ï¸ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        results = []
        
        for i, item in enumerate(items, 1):
            title = item.title.get_text()
            link = item.link.get_text()
            pub_date = item.pubDate.get_text()
            
            # description (ì œëª© + ì²« ë¬¸ì¥ ì „ëµ ìœ ì§€)
            description = ""
            if item.description:
                desc_soup = BeautifulSoup(item.description.get_text(), 'html.parser')
                description = desc_soup.get_text(strip=True)[:200]
            
            print(f"\nğŸ“° [{i}] {title}")
            print(f"    ğŸ• {pub_date}")
            
            analysis = analyzer.analyze_news(title, description)
            
            print(f"\n    ğŸ¤– AI ë¶„ì„ ê²°ê³¼:")
            print(f"       ê°ì„±: {analysis['ê°ì„±']} (ì‹ ë¢°ë„: {analysis['ê°ì„±_ì‹ ë¢°ë„']})")
            print(f"       ê¸ì • í‚¤ì›Œë“œ: {analysis['ê¸ì •í‚¤ì›Œë“œ']}ê°œ | ë¶€ì • í‚¤ì›Œë“œ: {analysis['ë¶€ì •í‚¤ì›Œë“œ']}ê°œ")
            print(f"       ğŸ’¡ ì¶”ì²œ: {analysis['ê±°ë˜ì‹ í˜¸']} (ì‹ ë¢°ë„: {analysis['ì‹ í˜¸_ì‹ ë¢°ë„']})")
            
            results.append({
                'ìˆœë²ˆ': i,
                'ê¸°ì—…': company,
                'ì œëª©': title,
                'ë‚ ì§œ': pub_date,
                'ë§í¬': link,
                **analysis
            })
        
        # ================= ì¢…í•© íŒë‹¨ =================
        print(f"\n{'='*70}")
        print("ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼")
        print(f"{'='*70}")
        
        df = pd.DataFrame(results)
        
        sentiment_counts = df['ê°ì„±'].value_counts()
        signal_counts = df['ê±°ë˜ì‹ í˜¸'].value_counts()
        
        print(f"\nê°ì„± ë¶„í¬: {dict(sentiment_counts)}")
        print(f"ì‹ í˜¸ ë¶„í¬: {dict(signal_counts)}")
        
        positive_ratio = sentiment_counts.get('ê¸ì •', 0) / len(df)
        negative_ratio = sentiment_counts.get('ë¶€ì •', 0) / len(df)
        
        if positive_ratio >= 0.6:
            final_recommendation = "ğŸ“ˆ ë§¤ìˆ˜ ê²€í†  ê¶Œì¥"
        elif negative_ratio >= 0.6:
            final_recommendation = "ğŸ“‰ ë§¤ë„ ë˜ëŠ” ê´€ë§ ê¶Œì¥"
        else:
            final_recommendation = "âš–ï¸ ì‹ ì¤‘í•œ ê´€ë§ ê¶Œì¥"
        
        print(f"\nğŸ¯ ìµœì¢… ì¶”ì²œ: {final_recommendation}")
        print(f"{'='*70}")
        
        # ================= ê²°ê³¼ ì €ì¥ =================
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        
        csv_file = f"{company}_analysis_{timestamp}.csv"
        json_file = f"{company}_analysis_{timestamp}.json"
        
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        df.to_json(json_file, orient="records", force_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ CSV ì €ì¥:  {csv_file}")
        print(f"ğŸ’¾ JSON ì €ì¥: {json_file}")
        
        return df
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# =========================================================
# 4. ë©€í‹° ê¸°ì—… ë™ì‹œ ë¶„ì„
# =========================================================
def analyze_multiple_companies(companies, max_news=3):
    print("ğŸ¯ ë©€í‹° ê¸°ì—… ë¶„ì„ ì‹œì‘\n")
    
    all_results = {}
    
    for company in companies:
        result = collect_and_analyze_latest_news(company, max_news)
        if result is not None:
            all_results[company] = result
    
    print(f"\n{'='*70}")
    print("ğŸ“Š ê¸°ì—…ë³„ ë¹„êµ ë¦¬í¬íŠ¸")
    print(f"{'='*70}\n")
    
    for company, df in all_results.items():
        positive = len(df[df['ê°ì„±'] == 'ê¸ì •'])
        buy_signals = len(df[df['ê±°ë˜ì‹ í˜¸'] == 'ë§¤ìˆ˜'])
        
        print(f"{company:15s} | ê¸ì •ë‰´ìŠ¤: {positive}/{len(df)} | ë§¤ìˆ˜ì‹ í˜¸: {buy_signals}/{len(df)}")
    
    print(f"{'='*70}")

# =========================================================
# 5. ì‹¤í–‰
# =========================================================
if __name__ == "__main__":
    # ë‹¨ì¼ ê¸°ì—…
    collect_and_analyze_latest_news("ì‚¼ì„±ì „ì", max_news=5)
    
    # ì—¬ëŸ¬ ê¸°ì—… ë¹„êµ ì˜ˆì‹œ
    # analyze_multiple_companies(["ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "ë„¤ì´ë²„"], max_news=3)
