import pandas as pd
import numpy as np
import joblib
import requests
from bs4 import BeautifulSoup
import urllib.parse
from datetime import datetime
import yfinance as yf

# âœ… 1. ëª¨ë¸ ë¡œë“œ
class NewsAnalyzer:
    def __init__(self):
        """í•™ìŠµëœ ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë“œ"""
        print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            self.sentiment_model = joblib.load('sentiment_model.pkl')
            self.trading_model = joblib.load('trading_model.pkl')
            self.tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')
            self.trading_vectorizer = joblib.load('trading_vectorizer.pkl')
            self.preprocessor = joblib.load('text_preprocessor.pkl')
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except FileNotFoundError:
            print("âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ë¨¼ì € í•™ìŠµì„ ì§„í–‰í•˜ì„¸ìš”.")
            raise
        
        # í‹°ì»¤ ë§¤í•‘
        self.ticker_map = {
            "ì‚¼ì„±ì „ì": "005930.KS",
            "SKí•˜ì´ë‹‰ìŠ¤": "000660.KS",
            "LGì—ë„ˆì§€ì†”ë£¨ì…˜": "373220.KS",
            "í˜„ëŒ€ì°¨": "005380.KS",
            "ê¸°ì•„": "000270.KS",
            "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤": "207940.KS",
            "ì…€íŠ¸ë¦¬ì˜¨": "068270.KS",
            "ì¹´ì¹´ì˜¤": "035720.KS",
            "ë„¤ì´ë²„": "035420.KS",
            "POSCOí™€ë”©ìŠ¤": "005490.KS"
        }
    
    def analyze_news(self, title, content):
        """
        ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„
        
        Returns:
            dict: ê°ì„±, ì‹ ë¢°ë„, ê±°ë˜ì‹ í˜¸ ë“±
        """
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        combined_text = title + ' ' + content
        cleaned = self.preprocessor.clean_text(combined_text)
        keywords = self.preprocessor.extract_keywords(cleaned)
        pos_count, neg_count = self.preprocessor.add_sentiment_features(cleaned)
        
        # ê°ì„± ë¶„ì„
        X_tfidf = self.tfidf_vectorizer.transform([keywords])
        X_extra = np.array([[pos_count, neg_count]])
        X_sentiment = np.hstack([X_tfidf.toarray(), X_extra])
        
        sentiment = self.sentiment_model.predict(X_sentiment)[0]
        sentiment_proba = self.sentiment_model.predict_proba(X_sentiment)[0]
        sentiment_confidence = max(sentiment_proba)
        
        # ê±°ë˜ ì‹ í˜¸ ì˜ˆì¸¡
        X_trading_tfidf = self.trading_vectorizer.transform([keywords])
        # ì„ì‹œ ìˆ˜ìµë¥  (ì‹¤ì œë¡  ê³¼ê±° ë°ì´í„° ì‚¬ìš©)
        X_trading_extra = np.array([[pos_count, neg_count, 0, 0]])
        X_trading = np.hstack([X_trading_tfidf.toarray(), X_trading_extra])
        
        trade_signal = self.trading_model.predict(X_trading)[0]
        trade_proba = self.trading_model.predict_proba(X_trading)[0]
        trade_confidence = max(trade_proba)
        
        return {
            'ê°ì„±': sentiment,
            'ê°ì„±_ì‹ ë¢°ë„': f"{sentiment_confidence:.1%}",
            'ê¸ì •í‚¤ì›Œë“œ': pos_count,
            'ë¶€ì •í‚¤ì›Œë“œ': neg_count,
            'ê±°ë˜ì‹ í˜¸': trade_signal,
            'ì‹ í˜¸_ì‹ ë¢°ë„': f"{trade_confidence:.1%}",
            'ë¶„ì„ì‹œê°': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
    
    def get_current_price(self, company):
        """í˜„ì¬ ì£¼ê°€ ì¡°íšŒ"""
        ticker = self.ticker_map.get(company)
        if not ticker:
            return None
        
        try:
            stock = yf.Ticker(ticker)
            current_price = stock.info.get('currentPrice', 
                            stock.info.get('regularMarketPrice', 0))
            prev_close = stock.info.get('previousClose', 0)
            
            if prev_close:
                change_pct = ((current_price - prev_close) / prev_close) * 100
            else:
                change_pct = 0
            
            return {
                'í˜„ì¬ê°€': f"{current_price:,.0f}ì›",
                'ì „ì¼ëŒ€ë¹„': f"{change_pct:+.2f}%"
            }
        except:
            return None

# âœ… 2. ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ & ë¶„ì„
def collect_and_analyze_latest_news(company, max_news=5):
    """íŠ¹ì • ê¸°ì—…ì˜ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ” [{company}] ìµœì‹  ë‰´ìŠ¤ ë¶„ì„")
    print(f"{'='*70}")
    
    analyzer = NewsAnalyzer()
    
    # í˜„ì¬ ì£¼ê°€ ì •ë³´
    price_info = analyzer.get_current_price(company)
    if price_info:
        print(f"\nğŸ“Š í˜„ì¬ ì£¼ê°€: {price_info['í˜„ì¬ê°€']} ({price_info['ì „ì¼ëŒ€ë¹„']})")
    
    # ìµœì‹  ë‰´ìŠ¤ ê²€ìƒ‰
    search_keyword = f"{company} ìµœì‹ "
    encoded = urllib.parse.quote(search_keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded}&hl=ko&gl=KR&ceid=KR:ko"
    
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        res = requests.get(rss_url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.content, "xml")
        items = soup.find_all("item")[:max_news]
        
        if not items:
            print("âš ï¸ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        results = []
        
        for i, item in enumerate(items, 1):
            title = item.title.get_text()
            link = item.link.get_text()
            pub_date = item.pubDate.get_text()
            
            print(f"\nğŸ“° [{i}] {title}")
            print(f"    ğŸ• {pub_date}")
            print(f"    ğŸ”— {link[:80]}...")
            
            # ë³¸ë¬¸ ìˆ˜ì§‘ (ê°„ë‹¨ ë²„ì „)
            try:
                article_res = requests.get(link, headers=headers, timeout=5)
                article_soup = BeautifulSoup(article_res.text, 'html.parser')
                
                # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                content = ""
                for selector in ['#dic_area', '.article_body', 'article']:
                    element = article_soup.select_one(selector)
                    if element:
                        content = element.get_text(strip=True)[:1000]
                        break
                
                if not content:
                    content = title  # ë³¸ë¬¸ ì—†ìœ¼ë©´ ì œëª©ë§Œ ì‚¬ìš©
                
            except:
                content = title
            
            # AI ë¶„ì„
            analysis = analyzer.analyze_news(title, content)
            
            print(f"\n    ğŸ¤– AI ë¶„ì„ ê²°ê³¼:")
            print(f"       ê°ì„±: {analysis['ê°ì„±']} (ì‹ ë¢°ë„: {analysis['ê°ì„±_ì‹ ë¢°ë„']})")
            print(f"       ê¸ì • í‚¤ì›Œë“œ: {analysis['ê¸ì •í‚¤ì›Œë“œ']}ê°œ | ë¶€ì • í‚¤ì›Œë“œ: {analysis['ë¶€ì •í‚¤ì›Œë“œ']}ê°œ")
            print(f"       ğŸ’¡ ì¶”ì²œ: {analysis['ê±°ë˜ì‹ í˜¸']} (ì‹ ë¢°ë„: {analysis['ì‹ í˜¸_ì‹ ë¢°ë„']})")
            
            results.append({
                'ìˆœë²ˆ': i,
                'ì œëª©': title,
                'ë‚ ì§œ': pub_date,
                'ë§í¬': link,
                **analysis
            })
        
        # ì¢…í•© íŒë‹¨
        print(f"\n{'='*70}")
        print("ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼")
        print(f"{'='*70}")
        
        df = pd.DataFrame(results)
        
        sentiment_counts = df['ê°ì„±'].value_counts()
        signal_counts = df['ê±°ë˜ì‹ í˜¸'].value_counts()
        
        print(f"\nê°ì„± ë¶„í¬: {dict(sentiment_counts)}")
        print(f"ì‹ í˜¸ ë¶„í¬: {dict(signal_counts)}")
        
        # ìµœì¢… ì¶”ì²œ
        if 'ê¸ì •' in sentiment_counts and sentiment_counts['ê¸ì •'] >= max_news * 0.6:
            final_recommendation = "ğŸ“ˆ ë§¤ìˆ˜ ê²€í†  ê¶Œì¥"
        elif 'ë¶€ì •' in sentiment_counts and sentiment_counts['ë¶€ì •'] >= max_news * 0.6:
            final_recommendation = "ğŸ“‰ ë§¤ë„ ë˜ëŠ” ê´€ë§ ê¶Œì¥"
        else:
            final_recommendation = "âš–ï¸ ì‹ ì¤‘í•œ ê´€ë§ ê¶Œì¥"
        
        print(f"\nğŸ¯ ìµœì¢… ì¶”ì²œ: {final_recommendation}")
        print(f"{'='*70}")
        
        # ê²°ê³¼ ì €ì¥
        output_file = f"{company}_analysis_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_file}")
        
        return df
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# âœ… 3. ë©€í‹° ê¸°ì—… ë™ì‹œ ë¶„ì„
def analyze_multiple_companies(companies, max_news=3):
    """ì—¬ëŸ¬ ê¸°ì—… ë™ì‹œ ë¶„ì„ ë° ë¹„êµ"""
    
    print("ğŸ¯ ë©€í‹° ê¸°ì—… ë¶„ì„ ì‹œì‘\n")
    
    all_results = {}
    
    for company in companies:
        result = collect_and_analyze_latest_news(company, max_news)
        if result is not None:
            all_results[company] = result
    
    # ë¹„êµ ë¦¬í¬íŠ¸
    print(f"\n{'='*70}")
    print("ğŸ“Š ê¸°ì—…ë³„ ë¹„êµ ë¦¬í¬íŠ¸")
    print(f"{'='*70}\n")
    
    for company, df in all_results.items():
        positive = len(df[df['ê°ì„±'] == 'ê¸ì •'])
        buy_signals = len(df[df['ê±°ë˜ì‹ í˜¸'] == 'ë§¤ìˆ˜'])
        
        print(f"{company:15s} | ê¸ì •ë‰´ìŠ¤: {positive}/{len(df)} | ë§¤ìˆ˜ì‹ í˜¸: {buy_signals}/{len(df)}")
    
    print(f"{'='*70}")

# âœ… 4. ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    # ë‹¨ì¼ ê¸°ì—… ë¶„ì„
    collect_and_analyze_latest_news("ì‚¼ì„±ì „ì", max_news=5)
    
    # ë˜ëŠ” ì—¬ëŸ¬ ê¸°ì—… ë™ì‹œ ë¶„ì„
    # analyze_multiple_companies(["ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "ë„¤ì´ë²„"], max_news=3)