import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import urllib.parse
from datetime import datetime
import re

# âœ… í¬ë¡¤ë§í•  ê¸°ì—… ë¦¬ìŠ¤íŠ¸
companies = [
    "ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "í˜„ëŒ€ì°¨", "ê¸°ì•„",
    "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "ì…€íŠ¸ë¦¬ì˜¨", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "POSCOí™€ë”©ìŠ¤"
]

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
}

# âœ… Google News RSSë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘
def collect_google_news(company, max_articles=1000):
    """Google News RSSë¡œ ë‰´ìŠ¤ ëŒ€ëŸ‰ ìˆ˜ì§‘"""
    
    print(f"\n{'='*60}")
    print(f"ğŸ” [{company}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ: {max_articles}ê°œ)")
    print(f"{'='*60}")
    
    all_news = []
    seen_titles = set()  # ì¤‘ë³µ ì œê±°ìš©
    
    # ì—¬ëŸ¬ ê²€ìƒ‰ì–´ ì¡°í•©ìœ¼ë¡œ ìˆ˜ì§‘ëŸ‰ ëŠ˜ë¦¬ê¸°
    search_keywords = [
        f"{company}",
        f"{company} ì£¼ê°€",
        f"{company} ì‹¤ì ",
        f"{company} ì˜ì—…ì´ìµ",
        f"{company} ë§¤ì¶œ",
        f"{company} íˆ¬ì",
    ]
    
    for keyword in search_keywords:
        if len(all_news) >= max_articles:
            break
        
        print(f"\n  ğŸ” ê²€ìƒ‰ì–´: '{keyword}'")
        
        encoded = urllib.parse.quote(keyword)
        rss_url = f"https://news.google.com/rss/search?q={encoded}&hl=ko&gl=KR&ceid=KR:ko"
        
        try:
            res = requests.get(rss_url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.content, "xml")
            
            items = soup.find_all("item")
            print(f"     ğŸ“° {len(items)}ê°œ ë°œê²¬")
            
            collected_this_search = 0
            
            for item in items:
                if len(all_news) >= max_articles:
                    break
                
                try:
                    title = item.title.get_text() if item.title else ""
                    pub_date = item.pubDate.get_text() if item.pubDate else ""
                    link = item.link.get_text() if item.link else ""
                    description = item.description.get_text() if item.description else ""
                    
                    # ì¤‘ë³µ ì œëª© ì œê±°
                    if title in seen_titles or len(title) < 10:
                        continue
                    
                    seen_titles.add(title)
                    
                    # descriptionì—ì„œ ë³¸ë¬¸ ìš”ì•½ ì¶”ì¶œ
                    if description:
                        desc_soup = BeautifulSoup(description, 'html.parser')
                        desc_text = desc_soup.get_text(strip=True)
                        summary = desc_text[:300] if len(desc_text) > 30 else title
                    else:
                        summary = title
                    
                    all_news.append({
                        "ê¸°ì—…": company,
                        "ì œëª©": title,
                        "ë‚ ì§œ": pub_date,
                        "ë§í¬": link,
                        "ë³¸ë¬¸ìš”ì•½": summary
                    })
                    
                    collected_this_search += 1
                    
                except Exception as e:
                    continue
            
            print(f"     âœ… {collected_this_search}ê°œ ìˆ˜ì§‘ (ëˆ„ì : {len(all_news)}ê°œ)")
            time.sleep(2)  # ê²€ìƒ‰ì–´ ê°„ ëŒ€ê¸°
            
        except Exception as e:
            print(f"     âŒ ì—ëŸ¬: {str(e)[:50]}")
            continue
    
    print(f"\n{'='*60}")
    print(f"âœ… [{company}] ì´ {len(all_news)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"{'='*60}")
    
    return all_news

# âœ… ë‹¤ìŒ ë‰´ìŠ¤ ì¶”ê°€ ìˆ˜ì§‘ (ë¶€ì¡±í•  ê²½ìš°)
def collect_daum_news(company, current_count, target=1000):
    """ë‹¤ìŒ ë‰´ìŠ¤ë¡œ ë¶€ì¡±ë¶„ ì±„ìš°ê¸°"""
    
    needed = target - current_count
    if needed <= 0:
        return []
    
    print(f"\n  âš ï¸ ë¶€ì¡±ë¶„ {needed}ê°œ - ë‹¤ìŒ ë‰´ìŠ¤ ì¶”ê°€ ìˆ˜ì§‘...")
    
    all_news = []
    encoded = urllib.parse.quote(company)
    
    # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ ìˆ˜ì§‘
    for page in range(1, 11):
        if len(all_news) >= needed:
            break
        
        url = f"https://search.daum.net/search?w=news&q={encoded}&sort=recency&p={page}"
        
        try:
            res = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            
            news_items = soup.select('div.c-item-content')
            
            for item in news_items:
                if len(all_news) >= needed:
                    break
                
                try:
                    title_tag = item.select_one('a.f_link_b')
                    if not title_tag:
                        continue
                    
                    title = title_tag.get_text(strip=True)
                    link = title_tag.get('href', '')
                    
                    summary_tag = item.select_one('p.c-summary')
                    summary = summary_tag.get_text(strip=True)[:300] if summary_tag else title
                    
                    date_tag = item.select_one('span.c-datetime')
                    pub_date = date_tag.get_text(strip=True) if date_tag else ""
                    
                    all_news.append({
                        "ê¸°ì—…": company,
                        "ì œëª©": title,
                        "ë‚ ì§œ": pub_date,
                        "ë§í¬": link,
                        "ë³¸ë¬¸ìš”ì•½": summary
                    })
                    
                except Exception:
                    continue
            
            time.sleep(1)
            
        except Exception:
            continue
    
    print(f"     âœ… ë‹¤ìŒ ë‰´ìŠ¤ {len(all_news)}ê°œ ì¶”ê°€ ìˆ˜ì§‘")
    return all_news

# âœ… ì „ì²´ ê¸°ì—… ìˆ˜ì§‘
def collect_all_companies():
    """ëª¨ë“  ê¸°ì—… ë‰´ìŠ¤ 1000ê°œì”© ìˆ˜ì§‘"""
    
    print("ğŸ¯ ëŒ€ëŸ‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ² ëª©í‘œ: ê¸°ì—…ë‹¹ 1,000ê°œ Ã— 10ê°œ ê¸°ì—… = ì´ 10,000ê°œ\n")
    
    all_data = []
    
    for i, company in enumerate(companies, 1):
        print(f"\n{'#'*60}")
        print(f"[{i}/{len(companies)}] {company} ìˆ˜ì§‘ ì‹œì‘")
        print(f"{'#'*60}")
        
        # 1ì°¨: Google News ìˆ˜ì§‘
        news = collect_google_news(company, max_articles=1000)
        
        # 2ì°¨: ë¶€ì¡±í•˜ë©´ ë‹¤ìŒ ë‰´ìŠ¤ ì¶”ê°€
        if len(news) < 1000:
            daum_news = collect_daum_news(company, len(news), target=1000)
            news.extend(daum_news)
        
        all_data.extend(news)
        
        print(f"\nğŸ“Š [{company}] ìµœì¢… ìˆ˜ì§‘: {len(news)}ê°œ")
        
        # ì¤‘ê°„ ì €ì¥ (ë§Œì•½ì„ ìœ„í•´)
        temp_df = pd.DataFrame(news)
        temp_file = f"{company}_news_temp.csv"
        temp_df.to_csv(temp_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì„ì‹œ ì €ì¥: {temp_file}")
        
        # ê¸°ì—… ê°„ ëŒ€ê¸°
        if i < len(companies):
            print(f"\nâ¸ï¸ ë‹¤ìŒ ê¸°ì—…ê¹Œì§€ 5ì´ˆ ëŒ€ê¸°...\n")
            time.sleep(5)
    
    # âœ… ìµœì¢… í†µí•© ì €ì¥
    if all_data:
        df = pd.DataFrame(all_data)
        
        filename = f"all_news_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"{'='*60}")
        print(f"ğŸ“Š ì´ {len(df):,}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        print(f"ğŸ’¾ íŒŒì¼: {filename}")
        print(f"\nğŸ“‹ ê¸°ì—…ë³„ ìˆ˜ì§‘ í˜„í™©:")
        print(df['ê¸°ì—…'].value_counts().sort_index())
        print(f"{'='*60}")
        
        # í†µê³„
        print(f"\nğŸ“ˆ ë°ì´í„° í†µê³„:")
        print(f"   - í‰ê·  ì œëª© ê¸¸ì´: {df['ì œëª©'].str.len().mean():.1f}ì")
        print(f"   - í‰ê·  ìš”ì•½ ê¸¸ì´: {df['ë³¸ë¬¸ìš”ì•½'].str.len().mean():.1f}ì")
        print(f"   - ì¤‘ë³µ ì œê±° í›„: {df.drop_duplicates(subset=['ì œëª©']).shape[0]:,}ê°œ")
        
        return df
    else:
        print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return None

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    start_time = time.time()
    
    result_df = collect_all_companies()
    
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")