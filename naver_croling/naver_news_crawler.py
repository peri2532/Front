"""
ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ - ë³‘ë ¬ ì²˜ë¦¬ ë²„ì „ (5ê°œ ë™ì‹œ ì‹¤í–‰)
ë©”ëª¨ë¦¬: 16GB ì´ìƒ ê¶Œì¥
"""
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, WebDriverException
from datetime import datetime, timedelta
import os
import re
from multiprocessing import Process, Queue
import traceback

class NaverNewsParallelCrawler:
    def __init__(self):
        self.companies = [
            'ì‚¼ì„±ì „ì', 'í˜„ëŒ€ìë™ì°¨', 'SKí•˜ì´ë‹‰ìŠ¤', 'LGì „ì', 'ë„¤ì´ë²„',
            'ì¹´ì¹´ì˜¤', 'ì‚¼ì„±SDI', 'í¬ìŠ¤ì½”', 'í˜„ëŒ€ì¤‘ê³µì—…', 'KBê¸ˆìœµ'
        ]
        
        os.makedirs('naver_news_data', exist_ok=True)
    
    @staticmethod
    def get_date_ranges(months=12):
        """ìµœê·¼ Nê°œì›”ì˜ ì›”ë³„ ë‚ ì§œ ë²”ìœ„ ìƒì„±"""
        today = datetime.now()
        date_ranges = []
        
        for i in range(months):
            if i == 0:
                end_date = today
                start_date = datetime(today.year, today.month, 1)
            else:
                target_date = today - timedelta(days=30*i)
                year = target_date.year
                month = target_date.month
                
                start_date = datetime(year, month, 1)
                
                if month == 12:
                    end_date = datetime(year, 12, 31)
                else:
                    end_date = datetime(year, month + 1, 1) - timedelta(days=1)
            
            date_ranges.append({
                'start': start_date.strftime('%Y.%m.%d'),
                'end': end_date.strftime('%Y.%m.%d'),
                'label': start_date.strftime('%Yë…„ %mì›”')
            })
        
        return date_ranges
    
    @staticmethod
    def is_valid_news_link(url, text):
        """ìœ íš¨í•œ ë‰´ìŠ¤ ë§í¬ì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        exclude_patterns = [
            'search.naver.com', 'keep.naver.com', 'media.naver.com/press',
            'javascript:', '#',
        ]
        
        for pattern in exclude_patterns:
            if pattern in url:
                return False
        
        if text and len(text.strip()) < 10:
            if '/article' in url or '/view' in url or '/news' in url:
                return True
            return False
        
        news_patterns = [
            'news.naver.com', 'n.news.naver.com', '/article',
            '/news/', '/view', 'articleView',
        ]
        
        for pattern in news_patterns:
            if pattern in url:
                return True
        
        if url.startswith('http') and any(domain in url for domain in ['.co.kr', '.com', '.kr']):
            if any(char.isdigit() for char in url):
                return True
        
        return False
    
    @staticmethod
    def crawl_single_company(company, articles_per_month=300, months=12, process_id=0):
        """ë‹¨ì¼ ê¸°ì—… í¬ë¡¤ë§ (í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰)"""
        try:
            print(f"\n[í”„ë¡œì„¸ìŠ¤ {process_id}] ğŸ¢ {company} ì‹œì‘")
            print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}] {'='*70}")
            
            # Chrome ì„¤ì •
            options = webdriver.ChromeOptions()
            options.add_argument('--start-maximized')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            options.add_argument('--page-load-strategy=eager')
            options.add_argument('--disable-gpu')
            options.add_argument('--no-sandbox')
            
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(8)
            
            # ë‚ ì§œ ë²”ìœ„
            date_ranges = NaverNewsParallelCrawler.get_date_ranges(months)
            
            print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}] ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {len(date_ranges)}ê°œì›”")
            
            all_articles = []
            
            # ì›”ë³„ ìˆ˜ì§‘
            for i, date_range in enumerate(date_ranges, 1):
                try:
                    print(f"\n[í”„ë¡œì„¸ìŠ¤ {process_id}] [{i}/{len(date_ranges)}] {date_range['label']}")
                    
                    # ê²€ìƒ‰
                    search_url = f"https://search.naver.com/search.naver?where=news&query={company}&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds={date_range['start']}&de={date_range['end']}"
                    driver.get(search_url)
                    time.sleep(3)
                    
                    # URL ìˆ˜ì§‘
                    articles = NaverNewsParallelCrawler.collect_urls(driver, articles_per_month, process_id)
                    
                    if articles:
                        for article in articles:
                            article['period'] = date_range['label']
                        all_articles.extend(articles)
                        print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]   âœ“ {len(articles)}ê°œ ìˆ˜ì§‘")
                    
                except Exception as e:
                    print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]   âš ï¸ {date_range['label']} ì‹¤íŒ¨: {e}")
                    continue
            
            if not all_articles:
                print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}] âŒ {company}: ê¸°ì‚¬ ì—†ìŒ")
                driver.quit()
                return
            
            print(f"\n[í”„ë¡œì„¸ìŠ¤ {process_id}] ğŸ“ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘ ({len(all_articles)}ê°œ)")
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            success = 0
            fail = 0
            
            for idx, article in enumerate(all_articles, 1):
                try:
                    content = NaverNewsParallelCrawler.extract_content(driver, article['url'])
                    article['content'] = content
                    
                    if content:
                        success += 1
                    else:
                        fail += 1
                    
                    if idx % 50 == 0:
                        print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]   â†’ {idx}/{len(all_articles)} (ì„±ê³µ: {success}, ì‹¤íŒ¨: {fail})")
                    
                    time.sleep(0.3)
                    
                except Exception as e:
                    article['content'] = ""
                    fail += 1
            
            # CSV ì €ì¥
            df = pd.DataFrame(all_articles)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"naver_news_data/{company}_monthly_{timestamp}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            print(f"\n[í”„ë¡œì„¸ìŠ¤ {process_id}] âœ… {company} ì™„ë£Œ!")
            print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]    ğŸ“Š {len(all_articles)}ê°œ")
            print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]    âœ“ ì„±ê³µ: {success}, âœ— ì‹¤íŒ¨: {fail}")
            print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]    ğŸ’¾ {filename}")
            
            # ì›”ë³„ í†µê³„
            print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]    ğŸ“ˆ ì›”ë³„ ë¶„í¬:")
            for period, count in df['period'].value_counts().sort_index(ascending=False).head(5).items():
                print(f"[í”„ë¡œì„¸ìŠ¤ {process_id}]       â€¢ {period}: {count}ê°œ")
            
            driver.quit()
            
        except Exception as e:
            print(f"\n[í”„ë¡œì„¸ìŠ¤ {process_id}] âŒ {company} ì „ì²´ ì‹¤íŒ¨: {e}")
            traceback.print_exc()
            try:
                driver.quit()
            except:
                pass
    
    @staticmethod
    def collect_urls(driver, target_count, process_id):
        """URL ìˆ˜ì§‘"""
        collected_urls = set()
        all_articles = []
        scroll_attempts = 0
        no_new_content = 0
        
        while len(all_articles) < target_count and scroll_attempts < 100:
            try:
                all_links = driver.find_elements(By.CSS_SELECTOR, 'ul.list_news a')
                
                new_articles = []
                for link in all_links:
                    try:
                        url = link.get_attribute('href')
                        text = link.text.strip()
                        title = link.get_attribute('title')
                        
                        if not NaverNewsParallelCrawler.is_valid_news_link(url, text):
                            continue
                        
                        if url in collected_urls:
                            continue
                        
                        if title and len(title) > 10:
                            final_title = title
                        elif text and len(text) > 10:
                            final_title = text
                        else:
                            continue
                        
                        collected_urls.add(url)
                        new_articles.append({'url': url, 'title': final_title})
                        
                    except:
                        continue
                
                all_articles.extend(new_articles)
                
                if new_articles:
                    no_new_content = 0
                else:
                    no_new_content += 1
                    if no_new_content >= 5:
                        break
                
                if len(all_articles) >= target_count:
                    break
                
                # ìŠ¤í¬ë¡¤
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                
                # ë”ë³´ê¸° ë²„íŠ¼
                try:
                    more_btn = driver.find_element(By.CSS_SELECTOR, 'a.btn_more, button.btn_more')
                    if more_btn.is_displayed():
                        more_btn.click()
                        time.sleep(2)
                except:
                    pass
                
                scroll_attempts += 1
                
            except Exception as e:
                break
        
        return all_articles[:target_count]
    
    @staticmethod
    def extract_content(driver, url):
        """ë³¸ë¬¸ ì¶”ì¶œ"""
        original_window = driver.current_window_handle
        
        try:
            driver.execute_script(f"window.open('{url}', '_blank');")
            WebDriverWait(driver, 3).until(lambda d: len(d.window_handles) > 1)
            driver.switch_to.window(driver.window_handles[-1])
            time.sleep(1)
            
            content_selectors = [
                'div#dic_area', 'div#articleBodyContents', 'div.article_body',
                'div.article_view', 'div.article-body', 'div.news_body',
                'div.view_body', 'article', 'div[itemprop="articleBody"]',
                'div.article', 'div.content', 'div.news_content',
            ]
            
            content_text = ""
            
            for selector in content_selectors:
                try:
                    content_elem = WebDriverWait(driver, 2).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                    )
                    text = content_elem.text.strip()
                    
                    if text and len(text) > 50:
                        content_text = text
                        break
                except:
                    continue
            
            if content_text:
                lines = []
                for line in content_text.split('\n'):
                    line = line.strip()
                    
                    if len(line) < 10:
                        continue
                    if 'ê¸°ì' in line and len(line) < 30:
                        continue
                    if re.match(r'^\d{4}[-./]\d{1,2}[-./]\d{1,2}', line):
                        continue
                    if line.startswith('[') and line.endswith(']'):
                        continue
                    if 'ë¬´ë‹¨ì „ì¬' in line or 'ì¬ë°°í¬' in line:
                        continue
                    
                    lines.append(line)
                    
                    if len(lines) >= 3:
                        break
                
                result = ' '.join(lines[:3])
                
                if len(result) > 300:
                    result = result[:300] + '...'
                
                driver.close()
                driver.switch_to.window(original_window)
                return result
            
            driver.close()
            driver.switch_to.window(original_window)
            return ""
            
        except:
            try:
                if len(driver.window_handles) > 1:
                    driver.close()
                driver.switch_to.window(original_window)
            except:
                pass
            return ""
    
    def crawl_all_companies_parallel(self, articles_per_month=300, months=12, parallel=5):
        """ë³‘ë ¬ë¡œ ëª¨ë“  ê¸°ì—… í¬ë¡¤ë§"""
        print(f"\n{'='*70}")
        print(f"ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ - ë³‘ë ¬ ì²˜ë¦¬")
        print(f"{'='*70}")
        print(f"ğŸ“‹ ìˆ˜ì§‘ ëŒ€ìƒ: {len(self.companies)}ê°œ ê¸°ì—…")
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {months}ê°œì›”")
        print(f"ğŸ¯ ì›”ë³„ ëª©í‘œ: {articles_per_month}ê°œ")
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬: {parallel}ê°œ ë™ì‹œ ì‹¤í–‰")
        print(f"ğŸ“Š ê¸°ì—…ë‹¹ ì´: ì•½ {articles_per_month * months}ê°œ")
        print(f"ğŸ’¾ ê¶Œì¥ ë©”ëª¨ë¦¬: 16GB ì´ìƒ\n")
        
        start_time = time.time()
        
        # ê¸°ì—…ì„ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        company_groups = []
        for i in range(0, len(self.companies), parallel):
            company_groups.append(self.companies[i:i+parallel])
        
        print(f"ğŸ“¦ {len(company_groups)}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„í•  (ê·¸ë£¹ë‹¹ {parallel}ê°œì”©)\n")
        
        # ê·¸ë£¹ë³„ë¡œ ë³‘ë ¬ ì‹¤í–‰
        for group_idx, company_group in enumerate(company_groups, 1):
            print(f"\n{'#'*70}")
            print(f"# ê·¸ë£¹ {group_idx}/{len(company_groups)}: {', '.join(company_group)}")
            print(f"{'#'*70}\n")
            
            processes = []
            
            # í”„ë¡œì„¸ìŠ¤ ì‹œì‘
            for idx, company in enumerate(company_group):
                p = Process(
                    target=self.crawl_single_company,
                    args=(company, articles_per_month, months, idx+1)
                )
                p.start()
                processes.append(p)
                time.sleep(2)  # í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ê°„ê²©
            
            # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
            for p in processes:
                p.join()
            
            print(f"\nâœ… ê·¸ë£¹ {group_idx} ì™„ë£Œ!\n")
            
            if group_idx < len(company_groups):
                print(f"â³ ë‹¤ìŒ ê·¸ë£¹ê¹Œì§€ 10ì´ˆ ëŒ€ê¸°...\n")
                time.sleep(10)
        
        elapsed_time = time.time() - start_time
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time/60:.1f}ë¶„ ({elapsed_time/3600:.1f}ì‹œê°„)")
        print(f"{'='*70}\n")
        
        # ê²°ê³¼ íŒŒì¼ í™•ì¸
        print("ğŸ“Š ìƒì„±ëœ íŒŒì¼:")
        import glob
        files = glob.glob('naver_news_data/*_monthly_*.csv')
        for f in sorted(files):
            try:
                df = pd.read_csv(f)
                company_name = f.split('/')[-1].split('_monthly_')[0]
                print(f"  âœ“ {company_name}: {len(df)}ê°œ ê¸°ì‚¬")
            except:
                pass
        
        print(f"\nğŸ“ ì €ì¥ ìœ„ì¹˜: naver_news_data/")
        print(f"âš¡ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì‹œê°„ ë‹¨ì¶• ì™„ë£Œ!\n")

if __name__ == "__main__":
    print("="*70)
    print("âš ï¸  ë³‘ë ¬ ì²˜ë¦¬ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­:")
    print("="*70)
    print("1. RAM 16GB ì´ìƒ ê¶Œì¥")
    print("2. Chrome ë¸Œë¼ìš°ì € 5ê°œê°€ ë™ì‹œì— ì‹¤í–‰ë©ë‹ˆë‹¤")
    print("3. ì‘ì—… ê´€ë¦¬ìì—ì„œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ê¶Œì¥")
    print("4. ì‹¤í–‰ ì¤‘ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ ìµœì†Œí™” ê¶Œì¥")
    print("="*70)
    
    response = input("\nê³„ì† ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    
    if response.lower() != 'y':
        print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        exit()
    
    crawler = NaverNewsParallelCrawler()
    
    try:
        # ë³‘ë ¬ 5ê°œë¡œ ì‹¤í–‰
        crawler.crawl_all_companies_parallel(
            articles_per_month=300,
            months=12,
            parallel=5  # 5ê°œ ë™ì‹œ ì‹¤í–‰
        )
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
    finally:
        print("\nğŸ‘‹ í¬ë¡¤ëŸ¬ ì¢…ë£Œ")