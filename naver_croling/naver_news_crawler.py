"""
ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ - ë‹¬ë³„ ìˆ˜ì§‘ (ê°ì„±ë¶„ì„ìš©)
ì‹œê°„ì  ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ì›”ë³„ ë¶„í•  ìˆ˜ì§‘
"""
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, WebDriverException
from datetime import datetime, timedelta
import os
import re

class NaverNewsMonthlycrawler:
    def __init__(self):
        self.options = webdriver.ChromeOptions()
        self.options.add_argument('--start-maximized')
        self.options.add_argument('--disable-blink-features=AutomationControlled')
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)
        self.options.add_argument('--page-load-strategy=eager')
        
        self.driver = webdriver.Chrome(options=self.options)
        self.driver.set_page_load_timeout(20)
        
        self.companies = [
            'ì‚¼ì„±ì „ì', 'í˜„ëŒ€ìë™ì°¨', 'SKí•˜ì´ë‹‰ìŠ¤', 'LGì „ì', 'ë„¤ì´ë²„',
            'ì¹´ì¹´ì˜¤', 'ì‚¼ì„±SDI', 'í¬ìŠ¤ì½”', 'í˜„ëŒ€ì¤‘ê³µì—…', 'KBê¸ˆìœµ'
        ]
        
        os.makedirs('naver_news_data', exist_ok=True)
    
    def get_date_ranges(self, months=3):
        """ìµœê·¼ Nê°œì›”ì˜ ì›”ë³„ ë‚ ì§œ ë²”ìœ„ ìƒì„±"""
        today = datetime.now()
        date_ranges = []
        
        for i in range(months):
            # iê°œì›” ì „ì˜ ì²«ë‚ ê³¼ ë§ˆì§€ë§‰ë‚ 
            if i == 0:
                # ì´ë²ˆ ë‹¬: 1ì¼ ~ ì˜¤ëŠ˜
                end_date = today
                start_date = datetime(today.year, today.month, 1)
            else:
                # ì´ì „ ë‹¬ë“¤
                target_date = today - timedelta(days=30*i)
                year = target_date.year
                month = target_date.month
                
                # í•´ë‹¹ ì›”ì˜ ì²«ë‚ 
                start_date = datetime(year, month, 1)
                
                # í•´ë‹¹ ì›”ì˜ ë§ˆì§€ë§‰ë‚ 
                if month == 12:
                    end_date = datetime(year, 12, 31)
                else:
                    end_date = datetime(year, month + 1, 1) - timedelta(days=1)
            
            date_ranges.append({
                'start': start_date.strftime('%Y.%m.%d'),
                'end': end_date.strftime('%Y.%m.%d'),
                'label': start_date.strftime('%Yë…„ %mì›”')
            })
        
        return date_ranges
    
    def search_company_news_by_date(self, company, start_date, end_date, period_label):
        """íŠ¹ì • ê¸°ê°„ì˜ ê¸°ì—… ë‰´ìŠ¤ ê²€ìƒ‰"""
        try:
            print(f"  ğŸ—“ï¸  {period_label} ({start_date} ~ {end_date})")
            
            # ë‚ ì§œ ë²”ìœ„ê°€ í¬í•¨ëœ URL
            search_url = f"https://search.naver.com/search.naver?where=news&query={company}&sm=tab_opt&sort=1&photo=0&field=0&pd=3&ds={start_date}&de={end_date}"
            
            self.driver.get(search_url)
            time.sleep(3)
            
            print(f"  âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return False
    
    def is_valid_news_link(self, url, text):
        """ìœ íš¨í•œ ë‰´ìŠ¤ ë§í¬ì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        exclude_patterns = [
            'search.naver.com',
            'keep.naver.com',
            'media.naver.com/press',
            'javascript:',
            '#',
        ]
        
        for pattern in exclude_patterns:
            if pattern in url:
                return False
        
        if text and len(text.strip()) < 10:
            if '/article' in url or '/view' in url or '/news' in url:
                return True
            return False
        
        news_patterns = [
            'news.naver.com',
            'n.news.naver.com',
            '/article',
            '/news/',
            '/view',
            'articleView',
        ]
        
        for pattern in news_patterns:
            if pattern in url:
                return True
        
        if url.startswith('http') and any(domain in url for domain in ['.co.kr', '.com', '.kr']):
            if any(char.isdigit() for char in url):
                return True
        
        return False
    
    def extract_article_content(self, url, max_retries=2):
        """ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ 1-3ì¤„ ì¶”ì¶œ"""
        original_window = self.driver.current_window_handle
        
        for attempt in range(max_retries):
            try:
                self.driver.execute_script(f"window.open('{url}', '_blank');")
                WebDriverWait(self.driver, 5).until(lambda d: len(d.window_handles) > 1)
                self.driver.switch_to.window(self.driver.window_handles[-1])
                time.sleep(2)
                
                content_selectors = [
                    'div#dic_area', 'div#articleBodyContents', 'div.article_body',
                    'div#articeBody', 'div.article_view', 'div.article-body',
                    'div.news_body', 'div.view_body', 'div#news-body-area',
                    'div.news-article-body', 'article', 'div[itemprop="articleBody"]',
                    'div.article-text', 'div.article', 'div.content', 'div.news_content',
                ]
                
                content_text = ""
                
                for selector in content_selectors:
                    try:
                        content_elem = WebDriverWait(self.driver, 3).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        text = content_elem.text.strip()
                        
                        if text and len(text) > 50:
                            content_text = text
                            break
                    except:
                        continue
                
                if content_text:
                    lines = []
                    for line in content_text.split('\n'):
                        line = line.strip()
                        
                        if len(line) < 10:
                            continue
                        if 'ê¸°ì' in line and len(line) < 30:
                            continue
                        if re.match(r'^\d{4}[-./]\d{1,2}[-./]\d{1,2}', line):
                            continue
                        if line.startswith('[') and line.endswith(']'):
                            continue
                        if 'ë¬´ë‹¨ì „ì¬' in line or 'ì¬ë°°í¬' in line:
                            continue
                        
                        lines.append(line)
                        
                        if len(lines) >= 3:
                            break
                    
                    result = ' '.join(lines[:3])
                    
                    if len(result) > 300:
                        result = result[:300] + '...'
                    
                    self.driver.close()
                    self.driver.switch_to.window(original_window)
                    return result
                
                self.driver.close()
                self.driver.switch_to.window(original_window)
                return ""
                
            except:
                try:
                    if len(self.driver.window_handles) > 1:
                        self.driver.close()
                    self.driver.switch_to.window(original_window)
                except:
                    pass
                
                if attempt < max_retries - 1:
                    continue
                else:
                    return ""
        
        return ""
    
    def extract_articles_from_page(self):
        """í˜„ì¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ URLê³¼ ì œëª© ì¶”ì¶œ"""
        articles = []
        
        try:
            all_links = self.driver.find_elements(By.CSS_SELECTOR, 'ul.list_news a')
            
            if not all_links:
                return articles
            
            for link in all_links:
                try:
                    url = link.get_attribute('href')
                    text = link.text.strip()
                    title = link.get_attribute('title')
                    
                    if not self.is_valid_news_link(url, text):
                        continue
                    
                    if title and len(title) > 10:
                        final_title = title
                    elif text and len(text) > 10:
                        final_title = text
                    else:
                        continue
                    
                    articles.append({
                        'url': url,
                        'title': final_title
                    })
                    
                except Exception as e:
                    continue
        
        except Exception as e:
            print(f"    âŒ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return articles
    
    def scroll_and_collect(self, target_count=300):
        """ë¬´í•œ ìŠ¤í¬ë¡¤í•˜ë©° ê¸°ì‚¬ URL/ì œëª© ìˆ˜ì§‘"""
        collected_urls = set()
        all_articles = []
        scroll_attempts = 0
        max_no_new_content = 5
        no_new_content_count = 0
        
        print(f"  ğŸ“Š ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ (ëª©í‘œ: {target_count}ê°œ)")
        
        while len(all_articles) < target_count:
            articles = self.extract_articles_from_page()
            
            new_articles = []
            for article in articles:
                if article['url'] not in collected_urls:
                    collected_urls.add(article['url'])
                    new_articles.append(article)
            
            all_articles.extend(new_articles)
            
            if new_articles:
                print(f"    â†’ {len(new_articles)}ê°œ ì¶”ê°€ (ëˆ„ì : {len(all_articles)}ê°œ)")
                no_new_content_count = 0
            else:
                no_new_content_count += 1
                
                if no_new_content_count >= max_no_new_content:
                    print(f"    âš ï¸ ë” ì´ìƒ ê¸°ì‚¬ ì—†ìŒ ({len(all_articles)}ê°œë¡œ ì¢…ë£Œ)")
                    break
            
            if len(all_articles) >= target_count:
                print(f"    âœ… ëª©í‘œ ë‹¬ì„±!")
                break
            
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            try:
                more_btn = self.driver.find_element(By.CSS_SELECTOR, 'a.btn_more, button.btn_more')
                if more_btn.is_displayed():
                    more_btn.click()
                    time.sleep(2)
            except:
                pass
            
            scroll_attempts += 1
            
            if scroll_attempts > 100:
                print(f"    âš ï¸ ìµœëŒ€ ìŠ¤í¬ë¡¤ ë„ë‹¬ ({len(all_articles)}ê°œë¡œ ì¢…ë£Œ)")
                break
        
        return all_articles[:target_count]
    
    def extract_content_for_articles(self, articles):
        """ìˆ˜ì§‘í•œ ê¸°ì‚¬ë“¤ì˜ ë³¸ë¬¸ 1-3ì¤„ ì¶”ì¶œ"""
        print(f"\n  ğŸ“ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘ ({len(articles)}ê°œ)")
        
        success_count = 0
        fail_count = 0
        
        for i, article in enumerate(articles, 1):
            try:
                content = self.extract_article_content(article['url'])
                
                if content:
                    article['content'] = content
                    success_count += 1
                else:
                    article['content'] = ""
                    fail_count += 1
                
                if i % 20 == 0:
                    print(f"    â†’ {i}/{len(articles)} (ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count})")
                
                time.sleep(0.5)
                
            except Exception as e:
                article['content'] = ""
                fail_count += 1
        
        print(f"  âœ… ì™„ë£Œ! (ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count})")
        
        return articles
    
    def crawl_company_news_monthly(self, company, articles_per_month=300, months=3):
        """íŠ¹ì • ê¸°ì—…ì˜ ì›”ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print(f"\n{'='*70}")
        print(f"ğŸ¢ {company}")
        print(f"{'='*70}")
        
        # ë‚ ì§œ ë²”ìœ„ ìƒì„±
        date_ranges = self.get_date_ranges(months)
        
        print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„:")
        for dr in date_ranges:
            print(f"   â€¢ {dr['label']}: {dr['start']} ~ {dr['end']}")
        print()
        
        all_articles = []
        
        try:
            for i, date_range in enumerate(date_ranges, 1):
                print(f"\n[{i}/{len(date_ranges)}] {date_range['label']}")
                print("-" * 70)
                
                # í•´ë‹¹ ê¸°ê°„ìœ¼ë¡œ ê²€ìƒ‰
                if not self.search_company_news_by_date(
                    company, 
                    date_range['start'], 
                    date_range['end'],
                    date_range['label']
                ):
                    print(f"  âŒ {date_range['label']} ê²€ìƒ‰ ì‹¤íŒ¨")
                    continue
                
                # URL/ì œëª© ìˆ˜ì§‘
                articles = self.scroll_and_collect(articles_per_month)
                
                if not articles:
                    print(f"  âš ï¸ {date_range['label']} ê¸°ì‚¬ ì—†ìŒ")
                    continue
                
                # ì›”ë³„ í‘œì‹œ ì¶”ê°€
                for article in articles:
                    article['period'] = date_range['label']
                
                all_articles.extend(articles)
                
                print(f"  âœ“ {date_range['label']}: {len(articles)}ê°œ ìˆ˜ì§‘")
            
            if not all_articles:
                print(f"\nâŒ {company}: ì „ì²´ ê¸°ê°„ ê¸°ì‚¬ ì—†ìŒ")
                return None
            
            print(f"\n{'='*70}")
            print(f"ğŸ“Š {company} URL ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ")
            print(f"{'='*70}")
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            all_articles = self.extract_content_for_articles(all_articles)
            
            # CSV ì €ì¥
            df = pd.DataFrame(all_articles)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"naver_news_data/{company}_monthly_{timestamp}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            print(f"\nâœ… {company} ì™„ë£Œ!")
            print(f"   ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬")
            print(f"   ğŸ’¾ {filename}\n")
            
            # ì›”ë³„ í†µê³„
            print(f"   ğŸ“ˆ ì›”ë³„ ë¶„í¬:")
            for period in df['period'].value_counts().sort_index(ascending=False).items():
                print(f"      â€¢ {period[0]}: {period[1]}ê°œ")
            
            return df
                
        except Exception as e:
            print(f"\nâŒ {company} ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def crawl_all_companies(self, articles_per_month=300, months=3):
        """ëª¨ë“  ê¸°ì—…ì˜ ì›”ë³„ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print(f"\nğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ - ì›”ë³„ ìˆ˜ì§‘")
        print(f"ğŸ“‹ ìˆ˜ì§‘ ëŒ€ìƒ: {len(self.companies)}ê°œ ê¸°ì—…")
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {months}ê°œì›”")
        print(f"ğŸ¯ ì›”ë³„ ëª©í‘œ: {articles_per_month}ê°œ")
        print(f"ğŸ“Š ê¸°ì—…ë‹¹ ì´: ì•½ {articles_per_month * months}ê°œ\n")
        
        results = {}
        start_time = time.time()
        
        for i, company in enumerate(self.companies, 1):
            print(f"\n{'#'*70}")
            print(f"# [{i}/{len(self.companies)}] {company} ì‹œì‘")
            print(f"{'#'*70}")
            
            df = self.crawl_company_news_monthly(company, articles_per_month, months)
            results[company] = df
            
            if i < len(self.companies):
                print(f"\nâ³ ë‹¤ìŒ ê¸°ì—…ê¹Œì§€ 10ì´ˆ ëŒ€ê¸°...\n")
                time.sleep(10)
        
        elapsed_time = time.time() - start_time
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
        print(f"{'='*70}\n")
        
        print("ğŸ“Š ìµœì¢… ê²°ê³¼:")
        total = 0
        for company, df in results.items():
            if df is not None:
                count = len(df)
                total += count
                print(f"  âœ“ {company}: {count}ê°œ")
            else:
                print(f"  âœ— {company}: ì‹¤íŒ¨")
        
        print(f"\nğŸ’¾ ì´ {total}ê°œ ê¸°ì‚¬")
        print(f"ğŸ“ ì €ì¥: naver_news_data/")
        print(f"ğŸ“… ì‹œê°„ì  ë‹¤ì–‘ì„± í™•ë³´! (ì›”ë³„ ê· ë“± ë¶„í¬)\n")
        
        return results
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        self.driver.quit()

if __name__ == "__main__":
    crawler = NaverNewsMonthlycrawler()
    
    try:
        # ìµœê·¼ 3ê°œì›”, ì›”ë³„ 300ê°œì”© ìˆ˜ì§‘
        # ì´ 900ê°œ (ì‹œê°„ì ìœ¼ë¡œ ê· ë“± ë¶„í¬)
        results = crawler.crawl_all_companies(
            articles_per_month=300,  # ì›”ë³„ ê°œìˆ˜
            months=3                 # ìˆ˜ì§‘ ê°œì›” ìˆ˜
        )
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    finally:
        crawler.close()
        print("\nğŸ‘‹ í¬ë¡¤ëŸ¬ ì¢…ë£Œ")